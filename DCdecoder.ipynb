{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e629e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import math\n",
    "\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, num_features))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, num_features))\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        self.mean = torch.mean(x, dim=1, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = nn.Linear(c_in, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DACStructure(nn.Module):\n",
    "    def __init__(self, win_size, patch_size, channel, scale=None, attention_dropout=0.1):\n",
    "        super(DACStructure, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.window_size = win_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channel = channel\n",
    "\n",
    "    def forward(self, queries_patch_size, keys_patch_size, queries_patch_num, keys_patch_num, patch_index):\n",
    "        B, L_ps, H, E_ps = queries_patch_size.shape\n",
    "        _, L_pn, _, E_pn = queries_patch_num.shape\n",
    "\n",
    "        scale_patch_size = self.scale or 1. / math.sqrt(E_ps)\n",
    "        scores_patch_size = torch.einsum(\"blhe,bshe->bhls\", queries_patch_size, keys_patch_size)\n",
    "        attn_patch_size = scale_patch_size * scores_patch_size\n",
    "        series_patch_size = self.dropout(torch.softmax(attn_patch_size, dim=-1))\n",
    "        \n",
    "        current_patch_size = self.patch_size[patch_index]\n",
    "        series_patch_size = repeat(series_patch_size, 'b h m n -> b h (m p1) (n p2)', p1=current_patch_size, p2=current_patch_size)\n",
    "        \n",
    "        scale_patch_num = self.scale or 1. / math.sqrt(E_pn)\n",
    "        scores_patch_num = torch.einsum(\"blhe,bshe->bhls\", queries_patch_num, keys_patch_num)\n",
    "        attn_patch_num = scale_patch_num * scores_patch_num\n",
    "        series_patch_num = self.dropout(torch.softmax(attn_patch_num, dim=-1))\n",
    "        \n",
    "        num_patches = self.window_size // current_patch_size\n",
    "        series_patch_num = repeat(series_patch_num, 'b h m n -> b h (m p1) (n p2)', p1=num_patches, p2=num_patches)\n",
    "\n",
    "        return series_patch_size, series_patch_num\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.query_projection_patch_size = nn.Linear(d_model, d_model)\n",
    "        self.key_projection_patch_size = nn.Linear(d_model, d_model)\n",
    "        self.query_projection_patch_num = nn.Linear(d_model, d_model)\n",
    "        self.key_projection_patch_num = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = attention\n",
    "        \n",
    "        self.out_projection = nn.Linear(d_model, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model*2), nn.GELU(), nn.Linear(d_model*2, d_model))\n",
    "\n",
    "    def forward(self, x_patch_size, x_patch_num, patch_index):\n",
    "        B_ps, N_ps, D_ps = x_patch_size.shape\n",
    "        B_pn, N_pn, D_pn = x_patch_num.shape\n",
    "\n",
    "        queries_patch_size = self.query_projection_patch_size(x_patch_size).view(B_ps, N_ps, self.n_heads, -1)\n",
    "        keys_patch_size = self.key_projection_patch_size(x_patch_size).view(B_ps, N_ps, self.n_heads, -1)\n",
    "        \n",
    "        queries_patch_num = self.query_projection_patch_num(x_patch_num).view(B_pn, N_pn, self.n_heads, -1)\n",
    "        keys_patch_num = self.key_projection_patch_num(x_patch_num).view(B_pn, N_pn, self.n_heads, -1)\n",
    "\n",
    "        patch_wise, in_patch = self.attention(\n",
    "            queries_patch_size, keys_patch_size,\n",
    "            queries_patch_num, keys_patch_num,\n",
    "            patch_index\n",
    "        )\n",
    "        \n",
    "        return patch_wise, in_patch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "\n",
    "    def forward(self, x_patch_size, x_patch_num, patch_index):\n",
    "        for attn_layer in self.attn_layers:\n",
    "            patch_wise, in_patch = attn_layer(x_patch_size, x_patch_num, patch_index)\n",
    "        return patch_wise, in_patch\n",
    "\n",
    "class DCdetector(nn.Module):\n",
    "    def __init__(self, win_size, enc_in, n_heads, d_model, e_layers, patch_size, d_ff=None, dropout=0.1, channel=1): # channel 參數現在代表 enc_in\n",
    "        super(DCdetector, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.patch_size = patch_size\n",
    "        self.revin_layer = RevIN(num_features=enc_in)\n",
    "        self.channel = channel # 保存通道數\n",
    "\n",
    "        self.embedding_patch_size = nn.ModuleList()\n",
    "        self.embedding_patch_num = nn.ModuleList()\n",
    "        for i, p_size in enumerate(self.patch_size):\n",
    "            # 讓嵌入層能處理多通道。每個 patch 的輸入維度是 p_size * channel\n",
    "            self.embedding_patch_size.append(DataEmbedding(p_size * self.channel, d_model, dropout)) \n",
    "            \n",
    "            num_patches = self.win_size // p_size\n",
    "            # 數字嵌入層也需要考慮多通道\n",
    "            self.embedding_patch_num.append(DataEmbedding(num_patches * self.channel, d_model, dropout))\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                AttentionLayer(\n",
    "                    DACStructure(win_size, patch_size, channel, attention_dropout=dropout),\n",
    "                    d_model, n_heads\n",
    "                ) for l in range(e_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, M = x.shape # M 就是通道數\n",
    "        x = self.revin_layer(x, 'norm')\n",
    "        # x = x.permute(0, 2, 1) # 不再需要，或根據後續維度調整\n",
    "        # x = x.reshape(B * M, L, 1) # 【關鍵】移除這行！\n",
    "\n",
    "        patch_wise_final = []\n",
    "        in_patch_final = []\n",
    "\n",
    "        for i, p_size in enumerate(self.patch_size):\n",
    "            num_patches = self.win_size // p_size\n",
    "            \n",
    "            # 將 patch 和 channel 維度合併，送入 embedding 層\n",
    "            x_patch_size = rearrange(x, 'b (n p) m -> b n (p m)', p=p_size)\n",
    "            x_patch_size_emb = self.embedding_patch_size[i](x_patch_size)\n",
    "\n",
    "            x_patch_num = rearrange(x, 'b (n p) m -> b p (n m)', p=p_size)\n",
    "            x_patch_num_emb = self.embedding_patch_num[i](x_patch_num)\n",
    "\n",
    "            # Encoder 的輸入維度是正確的 (B, N, D_model)，不需更改\n",
    "            patch_wise, in_patch = self.encoder(x_patch_size_emb, x_patch_num_emb, i)\n",
    "            patch_wise_final.append(patch_wise)\n",
    "            in_patch_final.append(in_patch)\n",
    "\n",
    "        patch_wise_out = torch.stack(patch_wise_final, dim=0).mean(dim=0)\n",
    "        in_patch_out = torch.stack(in_patch_final, dim=0).mean(dim=0)\n",
    "        \n",
    "        # 【注意】這裡的 reshape 可能需要調整或移除，因為 M 維度已經被編碼到模型中了\n",
    "        # Anomaly Score 的計算可能也需要適應新的輸出形狀\n",
    "        # 最終的 anomaly score 應該是針對整個多通道樣本，而不是每個通道\n",
    "        # patch_wise_out = patch_wise_out.view(B, M, ... ) # <<<< 這行可能需要修改\n",
    "        # in_patch_out = in_patch_out.view(B, M, ... )   # <<<< 這行可能需要修改\n",
    "        \n",
    "        return patch_wise_out, in_patch_out\n",
    "\n",
    "class DCAgent:\n",
    "    def __init__(self, model, optimizer, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.history = list()\n",
    "    \n",
    "    def _calculate_loss(self, patch_wise_out, in_patch_out):\n",
    "        # ==================== MODIFICATION START ====================\n",
    "        # This section is modified based on a re-analysis of the paper's intent.\n",
    "        # The paper's Eq. (10) `L = (L_N - L_P)` is considered a typo, as it results in zero loss.\n",
    "        # The corrected, functional loss should be a sum, consistent with the paper's goal of\n",
    "        # measuring \"similarity\"  and the existence of experimental results.\n",
    "        # N: patch_wise_out (patch-wise representation)\n",
    "        # P: in_patch_out (in-patch representation)\n",
    "\n",
    "        # Pre-calculate distributions\n",
    "        N_dist = F.softmax(patch_wise_out, dim=-1)\n",
    "        log_N_dist = F.log_softmax(patch_wise_out, dim=-1)\n",
    "        N_dist_detached = N_dist.detach()\n",
    "        log_N_dist_detached = log_N_dist.detach()\n",
    "\n",
    "        P_dist = F.softmax(in_patch_out, dim=-1)\n",
    "        log_P_dist = F.log_softmax(in_patch_out, dim=-1)\n",
    "        P_dist_detached = P_dist.detach()\n",
    "        log_P_dist_detached = log_P_dist.detach()\n",
    "        \n",
    "        # Eq (8): L_P = sum(KL(P || Stopgrad(N)) + KL(Stopgrad(N) || P))\n",
    "        # This loss term only computes gradients for P (in_patch_out)\n",
    "        kl_P_Nsg = F.kl_div(log_N_dist_detached, P_dist, reduction='none').sum(dim=(-1, -2))\n",
    "        kl_Nsg_P = F.kl_div(log_P_dist, N_dist_detached, reduction='none').sum(dim=(-1, -2))\n",
    "        loss_p = kl_P_Nsg + kl_Nsg_P\n",
    "\n",
    "        # Eq (9): L_N = sum(KL(N || Stopgrad(P)) + KL(Stopgrad(P) || N))\n",
    "        # This loss term only computes gradients for N (patch_wise_out)\n",
    "        kl_N_Psg = F.kl_div(log_P_dist_detached, N_dist, reduction='none').sum(dim=(-1, -2))\n",
    "        kl_Psg_N = F.kl_div(log_N_dist, P_dist_detached, reduction='none').sum(dim=(-1, -2))\n",
    "        loss_n = kl_N_Psg + kl_Psg_N\n",
    "        \n",
    "        # Corrected Eq (10): L = (L_N + L_P) / len(N)\n",
    "        # Using addition (+) instead of subtraction (-) to create a meaningful, non-zero loss.\n",
    "        total_loss = (loss_n + loss_p).mean()\n",
    "\n",
    "        return total_loss\n",
    "        # ===================== MODIFICATION END =====================\n",
    "\n",
    "    def train(self, dataloader, epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x_batch, in dataloader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                patch_wise_out, in_patch_out = self.model(x_batch)\n",
    "                loss = self._calculate_loss(patch_wise_out, in_patch_out)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            self.history += [total_loss / len(dataloader)]\n",
    "            print(\"[Epoch {}] Loss = {}\".format(epoch+1, total_loss/len(dataloader)))\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "    def _calculate_anomaly_score(self, patch_wise_out, in_patch_out):\n",
    "        # This section literally implements Equation (11) from the paper.\n",
    "        # AnomalyScore(X) = sum(KL(P, Stopgrad(N)) + KL(N, Stopgrad(P)))\n",
    "        # During inference, Stopgrad (detach) has no effect on the value but makes the code match the paper.\n",
    "        B, M, L, _ = patch_wise_out.shape\n",
    "        scores = torch.zeros(B, M, L, device=self.device)\n",
    "\n",
    "        with torch.no_grad(): # Ensure no gradients are computed during scoring\n",
    "            for i in range(L):\n",
    "                # N is patch_wise_out, P is in_patch_out\n",
    "                p_i = F.softmax(patch_wise_out[:, :, i, :], dim=-1)\n",
    "                n_i = F.softmax(in_patch_out[:, :, i, :], dim=-1)\n",
    "                \n",
    "                # According to the paper's formula, which includes Stopgrad\n",
    "                p_i_detached = p_i.detach()\n",
    "                n_i_detached = n_i.detach()\n",
    "\n",
    "                # Term 1: KL(P, Stopgrad(N)) is implemented as KL(n_i || p_i_detached)\n",
    "                kl_n_p_sg = F.kl_div(p_i_detached.log(), n_i, reduction='none').sum(dim=-1)\n",
    "                \n",
    "                # Term 2: KL(N, Stopgrad(P)) is implemented as KL(p_i || n_i_detached)\n",
    "                kl_p_n_sg = F.kl_div(n_i_detached.log(), p_i, reduction='none').sum(dim=-1)\n",
    "                \n",
    "                # The total score is the sum\n",
    "                scores[:, :, i] = kl_n_p_sg + kl_p_n_sg\n",
    "        \n",
    "        return scores.permute(0,2,1)\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        self.model.eval()\n",
    "        all_scores = []\n",
    "        # No need for torch.no_grad() here as it's handled inside _calculate_anomaly_score\n",
    "        for x_batch, in dataloader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            patch_wise_out, in_patch_out = self.model(x_batch)\n",
    "            scores = self._calculate_anomaly_score(patch_wise_out, in_patch_out)\n",
    "            all_scores.append(scores.cpu())\n",
    "        \n",
    "        return torch.cat(all_scores, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70322b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_data(n_batch):\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    data = list()\n",
    "    for _ in range(n_batch):\n",
    "        arr = list()\n",
    "        scale = float(np.random.normal(60, 1, size = (1, )))\n",
    "        first_seg = (scale*np.random.normal(1, 0.02, size = (42, ))).tolist() \n",
    "\n",
    "        scale = float(np.random.normal(-10, 2, size = (1, )))\n",
    "        second_seg = (scale*(np.ones(shape = (50, )))).tolist()\n",
    "\n",
    "        scale = float(np.random.normal(70, 1, size = (1, )))\n",
    "        third_seg = (scale*(np.ones(shape = (13, )))).tolist()\n",
    "\n",
    "        arr += [first_seg + second_seg + third_seg]\n",
    "\n",
    "        scale = float(np.random.normal(70, 1, size = (1, )))\n",
    "        first_seg = (scale*(np.ones(shape = (42, )))).tolist()\n",
    "\n",
    "        scale = float(np.random.normal(60, 1, size = (1, )))\n",
    "        third_seg = (scale*np.random.normal(1, 0.02, size = (63, ))).tolist() \n",
    "        arr += [first_seg +  third_seg]\n",
    "\n",
    "        arr = np.concatenate([np.array(arr[0]).reshape(-1, 1), np.array(arr[1]).reshape(-1, 1)], axis = 1)\n",
    "        data += [arr.copy()]\n",
    "    return np.stack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056cb28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with paper-consistent (and corrected) loss function...\n",
      "[Epoch 1] Loss = 0.33005403727293015\n",
      "[Epoch 2] Loss = 0.3179282695055008\n",
      "[Epoch 3] Loss = 0.321370966732502\n",
      "[Epoch 4] Loss = 0.32408639788627625\n",
      "[Epoch 5] Loss = 0.3305903449654579\n",
      "[Epoch 6] Loss = 0.31553806364536285\n",
      "[Epoch 7] Loss = 0.3195965066552162\n",
      "[Epoch 8] Loss = 0.3176059424877167\n",
      "[Epoch 9] Loss = 0.315379336476326\n",
      "[Epoch 10] Loss = 0.32081206142902374\n",
      "[Epoch 11] Loss = 0.3221360370516777\n",
      "[Epoch 12] Loss = 0.3197200298309326\n",
      "[Epoch 13] Loss = 0.3240762874484062\n",
      "[Epoch 14] Loss = 0.3258325308561325\n",
      "[Epoch 15] Loss = 0.32299885898828506\n",
      "[Epoch 16] Loss = 0.31254640966653824\n",
      "[Epoch 17] Loss = 0.33241718262434006\n",
      "[Epoch 18] Loss = 0.31915922462940216\n",
      "[Epoch 19] Loss = 0.3169105499982834\n",
      "[Epoch 20] Loss = 0.31901200115680695\n",
      "[Epoch 21] Loss = 0.3134781941771507\n",
      "[Epoch 22] Loss = 0.3210374265909195\n",
      "[Epoch 23] Loss = 0.3176366686820984\n",
      "[Epoch 24] Loss = 0.31965383142232895\n",
      "[Epoch 25] Loss = 0.32019226998090744\n",
      "[Epoch 26] Loss = 0.32586587965488434\n",
      "[Epoch 27] Loss = 0.32016538828611374\n",
      "[Epoch 28] Loss = 0.3262430652976036\n",
      "[Epoch 29] Loss = 0.3224862962961197\n",
      "[Epoch 30] Loss = 0.3169466182589531\n",
      "[Epoch 31] Loss = 0.31453726440668106\n",
      "[Epoch 32] Loss = 0.32304565608501434\n",
      "[Epoch 33] Loss = 0.31922533363103867\n",
      "[Epoch 34] Loss = 0.32145218551158905\n",
      "[Epoch 35] Loss = 0.32198694348335266\n",
      "[Epoch 36] Loss = 0.3184971809387207\n",
      "[Epoch 37] Loss = 0.3168303072452545\n",
      "[Epoch 38] Loss = 0.3254793807864189\n",
      "[Epoch 39] Loss = 0.31133491545915604\n",
      "[Epoch 40] Loss = 0.32174355536699295\n",
      "[Epoch 41] Loss = 0.32877737283706665\n",
      "[Epoch 42] Loss = 0.32034819573163986\n",
      "[Epoch 43] Loss = 0.3144973963499069\n",
      "[Epoch 44] Loss = 0.308467760682106\n",
      "[Epoch 45] Loss = 0.3199620023369789\n",
      "[Epoch 46] Loss = 0.31957506388425827\n",
      "[Epoch 47] Loss = 0.31697244942188263\n",
      "[Epoch 48] Loss = 0.3197338879108429\n",
      "[Epoch 49] Loss = 0.3133275955915451\n",
      "[Epoch 50] Loss = 0.31315111368894577\n",
      "[Epoch 51] Loss = 0.3149357736110687\n",
      "[Epoch 52] Loss = 0.32692236453294754\n",
      "[Epoch 53] Loss = 0.32088442146778107\n",
      "[Epoch 54] Loss = 0.32091043144464493\n",
      "[Epoch 55] Loss = 0.32249345630407333\n",
      "[Epoch 56] Loss = 0.3208930939435959\n",
      "[Epoch 57] Loss = 0.3174424022436142\n",
      "[Epoch 58] Loss = 0.32454559206962585\n",
      "[Epoch 59] Loss = 0.31874121725559235\n",
      "[Epoch 60] Loss = 0.31172434985637665\n",
      "[Epoch 61] Loss = 0.32884378731250763\n",
      "[Epoch 62] Loss = 0.32061566412448883\n",
      "[Epoch 63] Loss = 0.3227953836321831\n",
      "[Epoch 64] Loss = 0.3252573162317276\n",
      "[Epoch 65] Loss = 0.311585508286953\n",
      "[Epoch 66] Loss = 0.32362716645002365\n",
      "[Epoch 67] Loss = 0.3201838508248329\n",
      "[Epoch 68] Loss = 0.3243167921900749\n",
      "[Epoch 69] Loss = 0.31151987612247467\n",
      "[Epoch 70] Loss = 0.32231948524713516\n",
      "[Epoch 71] Loss = 0.3284963294863701\n",
      "[Epoch 72] Loss = 0.3164714351296425\n",
      "[Epoch 73] Loss = 0.3160168454051018\n",
      "[Epoch 74] Loss = 0.3201392963528633\n",
      "[Epoch 75] Loss = 0.31918060779571533\n",
      "[Epoch 76] Loss = 0.31566956639289856\n",
      "[Epoch 77] Loss = 0.3152003511786461\n",
      "[Epoch 78] Loss = 0.3224729597568512\n",
      "[Epoch 79] Loss = 0.3182631805539131\n",
      "[Epoch 80] Loss = 0.3160198852419853\n",
      "[Epoch 81] Loss = 0.3100106194615364\n",
      "[Epoch 82] Loss = 0.3220638185739517\n",
      "[Epoch 83] Loss = 0.3227960392832756\n",
      "[Epoch 84] Loss = 0.31928586959838867\n",
      "[Epoch 85] Loss = 0.3126989156007767\n",
      "[Epoch 86] Loss = 0.3224127069115639\n",
      "[Epoch 87] Loss = 0.3127559572458267\n",
      "[Epoch 88] Loss = 0.32455769181251526\n",
      "[Epoch 89] Loss = 0.31784428656101227\n",
      "[Epoch 90] Loss = 0.31488241255283356\n",
      "[Epoch 91] Loss = 0.3192935287952423\n",
      "[Epoch 92] Loss = 0.317887082695961\n",
      "[Epoch 93] Loss = 0.31862741708755493\n",
      "[Epoch 94] Loss = 0.3241381570696831\n",
      "[Epoch 95] Loss = 0.3246670365333557\n",
      "[Epoch 96] Loss = 0.31827226281166077\n",
      "[Epoch 97] Loss = 0.3255687952041626\n",
      "[Epoch 98] Loss = 0.32212165743112564\n",
      "[Epoch 99] Loss = 0.32262660562992096\n",
      "[Epoch 100] Loss = 0.3209996223449707\n",
      "[Epoch 101] Loss = 0.3252819776535034\n",
      "[Epoch 102] Loss = 0.3183027431368828\n",
      "[Epoch 103] Loss = 0.3199000582098961\n",
      "[Epoch 104] Loss = 0.31149110198020935\n",
      "[Epoch 105] Loss = 0.3136911913752556\n",
      "[Epoch 106] Loss = 0.3141097277402878\n",
      "[Epoch 107] Loss = 0.31795693188905716\n",
      "[Epoch 108] Loss = 0.32004963606595993\n",
      "[Epoch 109] Loss = 0.32824137061834335\n",
      "[Epoch 110] Loss = 0.31840087473392487\n",
      "[Epoch 111] Loss = 0.3189186006784439\n",
      "[Epoch 112] Loss = 0.3231496512889862\n",
      "[Epoch 113] Loss = 0.31238121539354324\n",
      "[Epoch 114] Loss = 0.3201664611697197\n",
      "[Epoch 115] Loss = 0.32352466881275177\n",
      "[Epoch 116] Loss = 0.3218127563595772\n",
      "[Epoch 117] Loss = 0.32257695496082306\n",
      "[Epoch 118] Loss = 0.3114111125469208\n",
      "[Epoch 119] Loss = 0.3235592693090439\n",
      "[Epoch 120] Loss = 0.3169805109500885\n",
      "[Epoch 121] Loss = 0.3202250227332115\n",
      "[Epoch 122] Loss = 0.3263559937477112\n",
      "[Epoch 123] Loss = 0.3218533918261528\n",
      "[Epoch 124] Loss = 0.3220954015851021\n",
      "[Epoch 125] Loss = 0.32342226058244705\n",
      "[Epoch 126] Loss = 0.3234971910715103\n",
      "[Epoch 127] Loss = 0.3185446187853813\n",
      "[Epoch 128] Loss = 0.31929561495780945\n",
      "[Epoch 129] Loss = 0.3091645911335945\n",
      "[Epoch 130] Loss = 0.3170958608388901\n",
      "[Epoch 131] Loss = 0.3168623000383377\n",
      "[Epoch 132] Loss = 0.3123074546456337\n",
      "[Epoch 133] Loss = 0.3222673311829567\n",
      "[Epoch 134] Loss = 0.32258452475070953\n",
      "[Epoch 135] Loss = 0.3219469338655472\n",
      "[Epoch 136] Loss = 0.3171512261033058\n",
      "[Epoch 137] Loss = 0.3150486573576927\n",
      "[Epoch 138] Loss = 0.30929383635520935\n",
      "[Epoch 139] Loss = 0.3166395500302315\n",
      "[Epoch 140] Loss = 0.32106296718120575\n",
      "[Epoch 141] Loss = 0.31577118486166\n",
      "[Epoch 142] Loss = 0.3159109279513359\n",
      "[Epoch 143] Loss = 0.3184288740158081\n",
      "[Epoch 144] Loss = 0.325393445789814\n",
      "[Epoch 145] Loss = 0.31882212311029434\n",
      "[Epoch 146] Loss = 0.3162606284022331\n",
      "[Epoch 147] Loss = 0.3191811814904213\n",
      "[Epoch 148] Loss = 0.3024395629763603\n",
      "[Epoch 149] Loss = 0.3266274705529213\n",
      "[Epoch 150] Loss = 0.3179253116250038\n",
      "Training finished. Average loss: 0.3179\n",
      "\n",
      "Starting prediction with paper-consistent anomaly score...\n",
      "Prediction finished.\n",
      "Output anomaly scores shape: torch.Size([512, 105, 1])\n",
      "Expected shape: (512, 105, 2)\n",
      "\n",
      "Example anomaly score for first sample, first channel (first 10 points): \n",
      "tensor([4.3213e-07, 4.3213e-07, 4.3213e-07, 4.3213e-07, 4.3213e-07, 4.5821e-07,\n",
      "        4.5821e-07, 4.6194e-07, 4.6194e-07, 4.6194e-07])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    win_size = 105\n",
    "    patch_size = [5, 7]\n",
    "    enc_in = 2\n",
    "    d_model = 256\n",
    "    n_heads = 1\n",
    "    e_layers = 3\n",
    "    batch_size = 128\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = DCdetector(\n",
    "        win_size=win_size,\n",
    "        enc_in=enc_in,\n",
    "        n_heads=n_heads,\n",
    "        d_model=d_model,\n",
    "        e_layers=e_layers,\n",
    "        patch_size=patch_size,\n",
    "        channel=enc_in\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    agent = DCAgent(model, optimizer, device)\n",
    "    \n",
    "    # dummy_data = torch.randn(batch_size * 5, win_size, enc_in)\n",
    "    dummy_data = torch.tensor(mock_data(512), dtype = torch.float32)\n",
    "    dummy_dataset = torch.utils.data.TensorDataset(dummy_data)\n",
    "    dummy_dataloader = torch.utils.data.DataLoader(dummy_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(\"Starting training with paper-consistent (and corrected) loss function...\")\n",
    "    avg_loss = agent.train(dummy_dataloader, epochs = 150)\n",
    "    print(f\"Training finished. Average loss: {avg_loss[-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nStarting prediction with paper-consistent anomaly score...\")\n",
    "    anomaly_scores = agent.predict(dummy_dataloader)\n",
    "    print(\"Prediction finished.\")\n",
    "    print(f\"Output anomaly scores shape: {anomaly_scores.shape}\")\n",
    "    print(f\"Expected shape: {(dummy_data.shape[0], win_size, enc_in)}\")\n",
    "    \n",
    "    single_score_example = anomaly_scores[0, :, 0]\n",
    "    print(f\"\\nExample anomaly score for first sample, first channel (first 10 points): \\n{single_score_example[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3309f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_data_ts(n_batch):\n",
    "    import numpy as np\n",
    "    data = list()\n",
    "    for _ in range(n_batch):\n",
    "        arr = list()\n",
    "        scale = float(np.random.normal(60, 1, size = (1, )))\n",
    "        first_seg = (scale*np.random.normal(1, 0.02, size = (42, ))).tolist() \n",
    "\n",
    "        scale = float(np.random.normal(-10, 2, size = (1, )))\n",
    "        second_seg = (scale*(np.ones(shape = (50, )))).tolist()\n",
    "\n",
    "        scale = float(np.random.normal(70, 1, size = (1, )))\n",
    "        third_seg = (scale*(np.ones(shape = (13, )))).tolist()\n",
    "\n",
    "        arr += [first_seg + second_seg + third_seg]\n",
    "\n",
    "        scale = float(np.random.normal(60, 1, size = (1, )))\n",
    "        first_seg = (scale*np.random.normal(1, 0.02, size = (42, ))).tolist() \n",
    "\n",
    "        scale = float(np.random.normal(-10, 2, size = (1, )))\n",
    "        second_seg = (scale*(np.ones(shape = (50, )))).tolist()\n",
    "\n",
    "        scale = float(np.random.normal(70, 1, size = (1, )))\n",
    "        third_seg = (scale*(np.ones(shape = (13, )))).tolist()\n",
    "\n",
    "        arr += [first_seg + second_seg + third_seg]\n",
    "\n",
    "        arr = np.concatenate([np.array(arr[0]).reshape(-1, 1), np.array(arr[1]).reshape(-1, 1)], axis = 1)\n",
    "        data += [arr.copy()]\n",
    "    return np.stack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e13349c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example anomaly score for first sample, first channel (first 10 points): \n",
      "tensor([1.2666e-07, 1.2666e-07, 1.2666e-07, 1.2666e-07, 1.2666e-07, 2.4587e-07,\n",
      "        2.4587e-07, 1.2666e-07, 1.2666e-07, 1.2666e-07])\n",
      "\n",
      "Example anomaly score for first sample, first channel (first 10 points): \n",
      "tensor([8.7917e-07, 8.7917e-07, 8.7917e-07, 8.7917e-07, 8.7917e-07, 9.7230e-07,\n",
      "        9.7230e-07, 9.3505e-07, 9.3505e-07, 9.3505e-07])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dummy_data = torch.tensor(mock_data(1), dtype = torch.float32)\n",
    "dummy_dataset = torch.utils.data.TensorDataset(dummy_data)\n",
    "dummy_dataloader = torch.utils.data.DataLoader(dummy_dataset, batch_size=batch_size)\n",
    "anomaly_scores = agent.predict(dummy_dataloader)\n",
    "single_score_example = anomaly_scores[0, :, 0]\n",
    "plt.hist(anomaly_scores.reshape(-1, ).tolist(), label = \"normal\")\n",
    "print(f\"\\nExample anomaly score for first sample, first channel (first 10 points): \\n{single_score_example[:10]}\")\n",
    "\n",
    "dummy_data = torch.tensor(mock_data_ts(1), dtype = torch.float32)\n",
    "dummy_dataset = torch.utils.data.TensorDataset(dummy_data)\n",
    "dummy_dataloader = torch.utils.data.DataLoader(dummy_dataset, batch_size=batch_size)\n",
    "anomaly_scores = agent.predict(dummy_dataloader)\n",
    "single_score_example = anomaly_scores[0, :, 0]\n",
    "plt.hist(anomaly_scores.reshape(-1, ).tolist(), label = \"abnormal\")\n",
    "print(f\"\\nExample anomaly score for first sample, first channel (first 10 points): \\n{single_score_example[:10]}\")\n",
    "plt.grid(color = \"gray\", linestyle = \"--\", alpha = .4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420cc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 105, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.anomaly_scores.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
